{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1QICn7VJGqyI1rxXBerUPxM0K86WdTaX7","timestamp":1753839534797},{"file_id":"1B_DRZs3r17SKYsG40OS-riT8QhaNzUsl","timestamp":1753742850915},{"file_id":"1cYGIqVLDKwWxDFIk7pAaufO9ZjqNVfwl","timestamp":1751429226730},{"file_id":"1EW46_6JC_2yZ9a2HLm0NPI1v8GVoJdaY","timestamp":1729702351025},{"file_id":"1fACrJGx3uzMfpHdtpBA_ppZ61zr_vprs","timestamp":1696867805761}],"toc_visible":true,"authorship_tag":"ABX9TyMPtjqbOpLHYcrLBcPSCBfr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Data Encoding and Parameter**\n","In this lab, we will cover feature scaling through standardization and normalization, different feature encoding techniques, and hyperparameter turnning. We will also see firsthand how K-Folds Cross Validation aids in estimating the skill of ML models.<br>\n","\n","#### **Part 1:** [Data Scaling](#p1)\n","#### **Part 2:** [Data Encoding](#p2)\n","#### **Part 3:** [Ensemble Learning](#p3)\n","#### **Part 4:** [Hyperparameter Tunning](#p4)\n","#### **Part 5:** [Exercise](#p5)\n","\n","\n","\n"],"metadata":{"id":"MDuy6pKZsXwp"}},{"cell_type":"markdown","source":["### Mount Google Drive"],"metadata":{"id":"Ee86lpYYuwkS"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"KJuH8U_iGIvO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive"],"metadata":{"id":"YMKNPh3_sD30"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Import Needed Libraries"],"metadata":{"id":"eXI6RgJ8fO8d"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn import datasets, model_selection, metrics\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import *"],"metadata":{"id":"1NgOzaOxfhb9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Load Datasets"],"metadata":{"id":"TFAMkq7galOB"}},{"cell_type":"markdown","source":["####**Diabetes Dataset (`df_diabetes`)**"],"metadata":{"id":"AJ5eSdmAh3rz"}},{"cell_type":"code","source":["df_diabetes = pd.read_csv(\"datasets/diabetes.csv\")"],"metadata":{"id":"7foj6IEqh3UT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_diabetes.head()"],"metadata":{"id":"uR22X4d1_h9a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####**Breast Cander Dataset(`df_cancer`)**"],"metadata":{"id":"kkOPp6trfigL"}},{"cell_type":"code","source":["from sklearn.datasets import *\n","\n","data = load_breast_cancer()\n","\n","df_cancer = pd.DataFrame(data.data, columns=data.feature_names)\n","df_cancer['target'] = data.target"],"metadata":{"id":"PA491kMOfiVn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_cancer.head()"],"metadata":{"id":"xcqu0qik_pR3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####**Corp Dataset(`df_corp`)**"],"metadata":{"id":"Avuz9-L1axZG"}},{"cell_type":"code","source":["url = 'https://raw.githubusercontent.com/the-codingschool/TRAIN-datasets/main/crop_recommendation/crop%20recommendation%20clean.csv'\n","df_corp = pd.read_csv(url)"],"metadata":{"id":"W9c5BUoEaxc1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_corp.head()"],"metadata":{"id":"HCdXd5OR_ug1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####**Student Dataset(`df_student`)**\n","This dataset is used to predict a high school student's final grade according to a discrete category of 0 - 20. This dataset contains many features, as described below:\n","\n","* Medu : Mother's education status. 'none' for no education, 'primary' for through 4th grade, 'middleschool' for through 9th grade, 'highschool' for through 12th grade, 'higher' for anything over 12th grade.\n","* Fedu: Father's education status. Same categories as Medu.\n","* failures: How many classes the student has failed in the past.\n","* absences: The number of days the student has been absent.\n","* traveltime: How many minutes the student has to travel to get to school.\n","* goout: How the student has rated how often they go out with friends on a scale from 1 (very low) to 5 (very high).\n","* school: GP or MS are two different schools in Portugal where the data was collected.\n","* higher: Whether the student has expressed interested in taking higher education after graduating high school.\n","* famsize: GT3 for greater than 3 family members and LE3 for less than or equal to 3 family members.\n","* G3: The overall grade of the student at the end of the year."],"metadata":{"id":"B_zBK5ZAaxg7"}},{"cell_type":"code","source":["import pandas as pd\n","url = 'https://raw.githubusercontent.com/the-codingschool/TRAIN-datasets/main/student_portugal/student-por.csv'\n","df_student = pd.read_csv(url)\n","\n","edu_map = {0: 'none', 1: 'primary', 2: 'middleschool', 3: 'highschool', 4: 'higher'}\n","df_student['Medu'] = df_student['Medu'].map(edu_map)\n","df_student['Fedu'] = df_student['Fedu'].map(edu_map)\n","\n","df_student['traveltime'] *= 15\n","df_student['studytime'] *= 2.5\n","\n","selected_features = [\n","    'Medu',            # Mother's Education\n","    'Fedu',            # Father's Education\n","    'failures',        # Number of class failures\n","    'absences',        # Number of absences\n","    'traveltime',      # Travel time to school\n","    'goout',           # Going out with friends\n","    'school',       # School identifier\n","    'higher',      # Interest in higher education\n","    'famsize',      # Family size â‰¤ 3 members\n","    'G3'\n","]\n","\n","df_student = df_student[selected_features]\n"],"metadata":{"id":"Y_yFswf2by0q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_student.head()"],"metadata":{"id":"_TodXFLIocXi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p1\"></a>\n","#**Part 1: Data Scaling**\n","####The purpose of scaling is to ensure that all features have equal importance during the learning process and to avoid any bias that might arise due to differences in the scales of the features"],"metadata":{"id":"KteQhj1L-2Tl"}},{"cell_type":"markdown","source":["## Corp Data (df_corp)"],"metadata":{"id":"J0ys_-53DKE6"}},{"cell_type":"code","source":["df_corp.head()"],"metadata":{"id":"c0N0Q2l1nYHY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_corp.shape"],"metadata":{"id":"_cKmf19-SJqj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Retrieve data and labels\n","by tradition, use X for data, and y for labels. Column `crop` stores labels."],"metadata":{"id":"DBoWFyIw27RH"}},{"cell_type":"code","source":["#Get the data and labels\n"],"metadata":{"id":"RBoB_54z3vfk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Split the dataset into training and test datasets"],"metadata":{"id":"iCmVvJP13lri"}},{"cell_type":"code","source":["# split data into a training dataset and test dataset\n"],"metadata":{"id":"T7EkAXTpt5IP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pAkUqB0b5dBj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Scaling (Normalization)\n","Two popular data scaling methods include:\n","* Z-score normalization: return the standard score of a sample x as: z = (x - u) / s\n","* min-max normalization: transforms numerical data into a  range between 0 and 1\n"],"metadata":{"id":"iZIHfRuT5dy5"}},{"cell_type":"markdown","source":["### **StandardScaler**  (z-score normalization)\n","Here we create a z-score normalized version of the training and testing data using sklearn's `StandardScaler()`. We will use this to compare to the original training and test sets."],"metadata":{"id":"tZyvDzD66o-f"}},{"cell_type":"code","source":["std_scaler = StandardScaler()\n","# fit standardscaler based on X_train, and then apply standardscaler to X_train and X_test.\n","X_train_std = std_scaler.fit_transform(X_train)\n","X_test_std = std_scaler.transform(X_test)"],"metadata":{"id":"6w4C4Uog5d2r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **MinMaxScaler** (min-max normalization)\n","Here we create a min-max normalized  version of the training and testing data using sklearn's `MinMaxScaler()`. We will use this to compare to the original training and test sets"],"metadata":{"id":"w62BVhXD5d6Y"}},{"cell_type":"code","source":["#fit minmax Scaler base on X_train, and then apply the min-max scaler to X_train and X_test\n","norm_scaler = MinMaxScaler()\n","X_train_norm = norm_scaler.fit_transform(X_train)\n","X_test_norm = norm_scaler.transform(X_test)"],"metadata":{"id":"11M66LJ_7Lkb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualize Data\n","Before moving onto modeling, let's see if we can visually detect any differences between these types of scaling. Specifically, create three scatter plots as follows:\n","\n","One comparing the original training data's 0th column to the standardized data's 0th column.\n","Another comparing the original training data's 0th column to the normalized data's 0th column.\n","A third one comparing the standardized training data's 0th column to the normalized data's 0th column."],"metadata":{"id":"rRR2PoHc7Ln2"}},{"cell_type":"markdown","source":["### **1. Create a scatter plot comparing the original training data's 0th column to the standardized data's 0th column**."],"metadata":{"id":"Y0VlZdKn8fwj"}},{"cell_type":"code","source":[],"metadata":{"id":"YwiANtaZ8Npm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **2. Create a scatter plot comparing the original training data's 0th column to the *normalized* data's 0th column.**"],"metadata":{"id":"a13tlzx27LrQ"}},{"cell_type":"code","source":[],"metadata":{"id":"6yj3giOz823z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **3. Create a scatter plot comparing the *standardized* data's 0th column to the *normalized* data's 0th column.**"],"metadata":{"id":"HPb7BsOF827K"}},{"cell_type":"code","source":[],"metadata":{"id":"agH5qe1a9Jcx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Select Classification Model -- SVM Classifier\n","We build SVM models on both orginal dataset and scaled dataset"],"metadata":{"id":"VByZt1Od6AsO"}},{"cell_type":"code","source":["#Initialized a SVM model for original dataset\n"],"metadata":{"id":"IozD4eUr5_sA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Initialized a SVM model for z-score normalized dataset\n","\n"],"metadata":{"id":"NThHltlJd1Ro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train SVM models\n"],"metadata":{"id":"jd_ucXKJ6e6d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test SVM models and evlauate their performances\n"],"metadata":{"id":"qo3lEjsk6fKa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise"],"metadata":{"id":"f3VI37oDrO-V"}},{"cell_type":"markdown","source":["#### 1. Create SVM classifier on min-max normalized dataset, test it and evaluate its performance."],"metadata":{"id":"YcmpU2c8rSMm"}},{"cell_type":"code","source":[],"metadata":{"id":"R6AZ7RhyDdDn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. compare all three SVM models' performance. Be sure to use k-fold cross validation."],"metadata":{"id":"Qif2DwddrusL"}},{"cell_type":"code","source":[],"metadata":{"id":"ohZ-IkfyrSTE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p2\"></a>\n","# **Part 2: Feature Encoding**\n","#### Dealing with Attributes with String Values. Decision tree and other machine learning methods currently don't work with non-numeric attributes.\n","####Nominal and Ordinal attributes are handled differently.\n","*   **Ordinal attributes**: convert each attribute value to a number. Use sklearn `OrdinalEncoder()`\n","*   **Nominal attributes**: for each distinct value, create a binary\n","attribute to store it. To reduce redundance, create N-1 binary attributes for an attribute with N values.  Use sklearn `OneHotEncoder()`\n","\n"],"metadata":{"id":"xTvQqubaxDSt"}},{"cell_type":"markdown","source":["In this section, we will investigate the role that different forms of encoding have on a model's performance. Specifically, we will use KNN to predict a high school student's final grade according to a discrete category of 0 - 20. This dataset contains many features, as described below:\n","\n","* `Medu` : Mother's education status. 'none' for no education, 'primary' for through 4th grade, 'middleschool' for through 9th grade, 'highschool' for through 12th grade, 'higher' for anything over 12th grade.\n","* `Fedu`: Father's education status. Same categories as `Medu`.\n","* `failures`: How many classes the student has failed in the past.\n","* `absences`: The number of days the student has been absent.\n","* `traveltime`: How many minutes the student has to travel to get to school.\n","* `goout`: How the student has rated how often they go out with friends on a scale from 1 (very low) to 5 (very high).\n","* `school`: `GP` or `MS` are two different schools in Portugal where the data was collected.\n","* `higher`: Whether the student has expressed interested in taking higher education after graduating high school.\n","* `famsize`: `GT3` for greater than 3 family members and `LE3` for less than or equal to 3 family members.\n","* `G3`: The overall grade of the student at the end of the year, it will be the class label."],"metadata":{"id":"yR0KPaNRxE5w"}},{"cell_type":"markdown","source":["##Student dataset (`df_student`)"],"metadata":{"id":"V8GPu65b3J9V"}},{"cell_type":"markdown","source":["### Split the data into the training and test data"],"metadata":{"id":"Lz7CvffpxFAk"}},{"cell_type":"code","source":["features = df_student.drop(columns = 'G3')\n","label = df_student['G3']\n","\n","X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2, random_state=42)"],"metadata":{"id":"jds56UfcxFEF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Pick the classification model"],"metadata":{"id":"mN9ISbrjxFHO"}},{"cell_type":"markdown","source":["####To understand why we need encodings, try to construct a KNN model on the raw training set below"],"metadata":{"id":"9fu53kwuxFKV"}},{"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n","model = KNeighborsClassifier(n_neighbors = 5)\n","\n","model.fit(X_train, y_train)"],"metadata":{"id":"iHu52MhBxFNl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data Encoding\n","\n","Now, let's encode our categorical variables based on the type of variable they are,\n","\n","1. First, create a a version of `X_train` and `X_test` that only contains the numerical varibles from our dataset.\n","\n","2. Then, determine which categorical variables are ordered and which are unordered.\n","\n","3. Encode the ordered categorical variables using the ordinal encoder.\n","\n","4. Encode the unordered categorical variables using one hot encoding (dummy variable).\n","\n","<br>"],"metadata":{"id":"fT__nIpQ8Qz-"}},{"cell_type":"code","source":["#Run the cell below to see the subset of all categorical (`object` type) columns in this data frame.\n","columns_to_encode = df_student.select_dtypes(include = object).columns\n","\n","print(columns_to_encode)"],"metadata":{"id":"OeMhDMAS4i0B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **1. Create a version of `X_train` and `X_test` where the unencoded categorical variables are just dropped.**"],"metadata":{"id":"xuqawDyY4i3Z"}},{"cell_type":"code","source":["X_train_drop = X_train.drop(columns = columns_to_encode)\n","X_test_drop = X_test.drop(columns = columns_to_encode)\n","\n","X_train_drop.head()"],"metadata":{"id":"ODrfiPL34i6v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **2. Determine which categorical variables are ordered and which are unordered and create two lists that contain the feature names.**"],"metadata":{"id":"rmRXHruT4i-A"}},{"cell_type":"code","source":["ordered_features = ['Medu', 'Fedu']\n","unordered_features = ['school', 'higher', 'famsize']"],"metadata":{"id":"7YBZzCMq5ZxJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hp_RMpw_5Z04"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **3. Encode the ordered categorical variables using the ordinal encoder `OrdinalEncoder()`.**\n","**NOTE**: We must make copies of the dropped training and test sets using `.copy()` so that we can add the encoded columns without having to drop the original columns."],"metadata":{"id":"U1y1yEKY5Z4t"}},{"cell_type":"code","source":["from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n","X_train_enc = X_train_drop.copy()\n","X_test_enc = X_test_drop.copy()\n","\n","\n","ord_enc = OrdinalEncoder()\n","\n","X_train_enc[ordered_features] = ord_enc.fit_transform(X_train[ordered_features])\n","X_test_enc[ordered_features] = ord_enc.transform(X_test[ordered_features])\n","\n","X_train_enc.head()"],"metadata":{"id":"Oj_-tZLF5Z8c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Pm56kLwT5r_X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **4. Create a version of `X_train` and `X_test` where the unordered categorical variables are dummy variable encoded.**\n","\n","Since one hot encoding creates a new feature for every possible value of categorical features, the number of columns will grow dramatically. To account for this, we will break this process into two steps:\n","\n","1. Fit the one hot encoder to the training data and determine the new features.\n","\n","2. Transform (encode) the training and test sets accordingly.\n","\n","**Note** We must pass in the argument `drop = 'first'` to our OneHotEncoder to drop one of our encoded columns to make it dummy variable encoding."],"metadata":{"id":"wBO-7C4Q8g5E"}},{"cell_type":"code","source":["dv_enc = OneHotEncoder(sparse_output = False, drop = 'first')\n","dv_enc.set_output(transform = 'pandas')\n","\n","dv_enc.fit(X_train[unordered_features])\n","\n","dv_columns = dv_enc.get_feature_names_out()\n","print(dv_columns)"],"metadata":{"id":"5cX2ZhHz5sDB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_enc[dv_columns] = dv_enc.transform(X_train[unordered_features])\n","X_test_enc[dv_columns] = dv_enc.transform(X_test[unordered_features])\n","\n","X_train_enc.head()"],"metadata":{"id":"sFz_0A8e5sGi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_test_enc.head()"],"metadata":{"id":"twWxC0VC9txr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JuLZGUWsd2wO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **5. Build KNN (with k=5) on all attributes with categorical attributes encoded, and compare their performances .**\n"],"metadata":{"id":"_dHkrwLD5sQh"}},{"cell_type":"code","source":[],"metadata":{"id":"_MA58O4dJRb0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create confusion matrix and display it\n"],"metadata":{"id":"fVFLUmmAJRh3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **6. Build a Decision Tree on all attributes with categorical attributes encoded.**"],"metadata":{"id":"rYG0-EM3JRlM"}},{"cell_type":"code","source":[],"metadata":{"id":"VQh_mXlwLkX_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise -- Bank Dataset\n","In this exercise, you will need to\n","* Encode categorical attributes\n","* Pick a classification model to predict target variable y\n","\n","[More information on columns](https://archive.ics.uci.edu/dataset/222/bank+marketing)\n","\n"],"metadata":{"id":"MKyh3L8lPL7X"}},{"cell_type":"code","source":["# load the dataset\n","import pandas as pd\n","df_bank=pd.read_csv('datasets/bank.csv', sep=';')"],"metadata":{"id":"9N-obZn6PLsU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FS9YhadJPLn5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OmSC3dNuPLii"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p3\"> </a>\n","# **Part 3: Emsemble Learning**\n","---\n","**Ensemble Methods:** combining multiple machine learning models to create a stronger overall model. An ensemble of models generally performs better than any single constituent model for several reasons:\n","* Ensemble models reduce variance by averaging multiple models which helps avoid overfitting\n","* Different models can capture different patterns/relationships in the data\n","* Combining weak learner models can produce a strong overall model\n","\n","Some common ensemble methods include:\n","\n","* Bagging â€“ Training each model on a random subset of the data\n","* Boosting â€“ Training models sequentially, with each model focusing on the errors of the previous model\n","* Stacking â€“ Combining multiple models by using the predictions of base models as inputs to a meta model\n","* Voting Classifiers â€“ Combining models through averaging/majority voting on their predictions"],"metadata":{"id":"km6HtabkOxfV"}},{"cell_type":"markdown","source":["## **Random Forest -- Bagging**"],"metadata":{"id":"rJLJX5SLTZTd"}},{"cell_type":"markdown","source":["### Import library"],"metadata":{"id":"qGyJLof9EblG"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier"],"metadata":{"id":"GOX64_-QEZsq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **1.  Breast Cancer Dataset (`df_cancer`)**"],"metadata":{"id":"A2t8QoaCVZU7"}},{"cell_type":"code","source":["df_cancer.head()"],"metadata":{"id":"Pl6mtCzEVbnT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Split the data"],"metadata":{"id":"0VPcT80MD3LK"}},{"cell_type":"code","source":["X=df_cancer.drop(columns=['target'])\n","y=df_cancer['target']\n","X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"31iL-kb4D3HZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Train the model"],"metadata":{"id":"m-A6hMrLD3Dq"}},{"cell_type":"code","source":["rf=RandomForestClassifier()\n","rf.fit(X_train, y_train)\n","pred_rf=rf.predict(X_test)"],"metadata":{"id":"FX309HwZEsxe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Evaluate the Model Performance"],"metadata":{"id":"u7mWEgLgEs1S"}},{"cell_type":"code","source":["print(classification_report(y_test, pred_rf))"],"metadata":{"id":"MImLbNahEs4k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **2.  Diabetes Dataset (`df_diabetes`)**"],"metadata":{"id":"dJJ_8oRRfHLK"}},{"cell_type":"code","source":["df_diabetes.head()"],"metadata":{"id":"ct44VeOkfHen"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Split the dataset"],"metadata":{"id":"LzQgknkymJUD"}},{"cell_type":"code","source":["X=df_diabetes.drop(columns=['Outcome'])\n","y=df_diabetes.Outcome"],"metadata":{"id":"9IYOgxH3gKat"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split the dataset into training and testing\n","X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"Gv15lqgOgT2y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Train the Model"],"metadata":{"id":"mhv3D_0KmXsv"}},{"cell_type":"code","source":["# build the model\n","from sklearn.ensemble import RandomForestClassifier\n","rf_diabetes=RandomForestClassifier()\n","rf_diabetes.fit(X_train, y_train)\n","pred_d=rf_diabetes.predict(X_test)"],"metadata":{"id":"2k2SBa6pGDkx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Evaluate the Model Performance"],"metadata":{"id":"3S63H_hQmfom"}},{"cell_type":"code","source":["# model evaluation\n","print(classification_report(y_test, pred_d))"],"metadata":{"id":"fC9GmBtKGDtN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"puHGWiKUGD9u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Stacking**\n","Stacking works in two stages:\n","\n","*  Base Models: Train multiple models on the training data.\n","*  Meta-Model: Train a secondary model (meta-learner) on the predictions of the base models to make the final prediction."],"metadata":{"id":"ffTYgZNQLlMo"}},{"cell_type":"code","source":["#import libraries\n","from sklearn.ensemble import StackingClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.svm import SVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from xgboost import XGBClassifier"],"metadata":{"id":"FCUR2mKhvllO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Build base models"],"metadata":{"id":"2dOCg8IHRLWk"}},{"cell_type":"code","source":["base_models=[('rf', RandomForestClassifier()),\n","             ('nb', GaussianNB()),\n","             ('dt', DecisionTreeClassifier()),\n","             ('svm', SVC())]"],"metadata":{"id":"hiV_yYuLvlox"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Define meta model"],"metadata":{"id":"cfQt9mEJRSE5"}},{"cell_type":"code","source":["meta_model=LogisticRegression()"],"metadata":{"id":"nacW9qR0vlsN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Construct stacking clasifier"],"metadata":{"id":"UB3t-48zRajv"}},{"cell_type":"code","source":["stack_1 = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)\n"],"metadata":{"id":"0DGq_1gMvlvX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Train the stacking classifier and evaluate its performance"],"metadata":{"id":"GvcKWxP6Rp2F"}},{"cell_type":"code","source":["\n","\n"],"metadata":{"id":"_EFm0yDfzDJ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exercise"],"metadata":{"id":"V7TXcftGzDNo"}},{"cell_type":"markdown","source":["Build a different stacking strategy as follows:\n","* Use Naive Bayes and Decision Tree for base model\n","* **option 1:** create stacking using SVC for meta model\n","* **option 2:** craete stacking using logisticregression for meta model\n","* compare the performance of option 1 and option 2.  Using k-fold cross validation."],"metadata":{"id":"4kDGL3un0xHf"}},{"cell_type":"markdown","source":["####Create base models"],"metadata":{"id":"T3B9a2k25hwM"}},{"cell_type":"code","source":["# build the base model\n"],"metadata":{"id":"awgIQ2LI0xdo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### option 1: Use SVC for meta model, train the stacking classifier and evaluate it"],"metadata":{"id":"qhwN1g5R5olK"}},{"cell_type":"code","source":["# build meta model\n"],"metadata":{"id":"kR3LriuszDQ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#build stacking classifier, and evaluate it\n"],"metadata":{"id":"Yp1USJCx12pK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### option 2: Use Logistic regression for meta model, train the stacking classifier and evaluate it\n"],"metadata":{"id":"xkazgXFR3M_B"}},{"cell_type":"code","source":[],"metadata":{"id":"8GeCQgwGQWo5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Compare the performance of two stacking classifier"],"metadata":{"id":"wqRgzMmk3UPY"}},{"cell_type":"code","source":[],"metadata":{"id":"TBwGx0386q-g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **XGBoost -- boosting**"],"metadata":{"id":"NIe9sk_xTXj-"}},{"cell_type":"markdown","source":["### Import library"],"metadata":{"id":"0NysHjM3G84R"}},{"cell_type":"code","source":["from xgboost import XGBClassifier"],"metadata":{"id":"uIRqlilGTZPH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Diabetes Dataset"],"metadata":{"id":"Q7EPx9I-Vbvt"}},{"cell_type":"code","source":["df_diabetes.head()"],"metadata":{"id":"u4U3JiQxwGU8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Split the dataset"],"metadata":{"id":"HGM0p8OfHx1N"}},{"cell_type":"code","source":[],"metadata":{"id":"14-ZixOmH3U2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Build model"],"metadata":{"id":"qmAuiC-5H3ap"}},{"cell_type":"code","source":["\n","\n"],"metadata":{"id":"oJQOdZ4iIupN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Evaluate Model"],"metadata":{"id":"5pkue7EDIuu3"}},{"cell_type":"code","source":[],"metadata":{"id":"VF6h8kUsIuzR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Exercise\n","Apply XGBClassifier to breast cancer dataset (`df_cancer`)"],"metadata":{"id":"1E4A4H95wSDb"}},{"cell_type":"code","source":[],"metadata":{"id":"oMCZh8rt4hfD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-eW6NOWN4hj2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fdjOt2f5wSJI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p4\"> </a>\n","# **Part 4: Hyperparameter Tunning**\n","Hyper-parameters are parameters that are not directly learnt within estimators. They are set before the training of the model. In scikit-learn they are passed as arguments to the constructor of the estimator classes.\n","* [Motivational Example](#p31)\n","* [Hyperparameter Tunning for KNN](#p32)\n","* [Hyperparameter Tunning for Decision Tree](#p33)\n","* [Hyperparameter Tunning for Bayes Classifer](#p34)\n","* [Hyperparameter Tunning for Support Vector Machine (SVM)](#p35)\n"],"metadata":{"id":"emEqd6c0MuHH"}},{"cell_type":"markdown","source":["<a name=\"p31\"> </a>\n","## Motivational Example: How to find the best n_neighbors for a KNN"],"metadata":{"id":"XSadTix1MuL_"}},{"cell_type":"markdown","source":["### **1. Load the dataset-- Spotify Dataset**\n","---\n","Spotify is one of the most popular digital music streaming services with over 515 million monthly users. The following dataset from Spotify data looks at different qualities of songs like energy, key, loudness, and tempo to see if a song is a top or bottom hit.\n","\n","The features are as follows:\n","* `artist`: song artist(s)\n","* `song`: song title\n","* `duration_ms`: the track length in milliseconds (ms)\n","* `year`: the year the song was released\n","* `top half`: whether or not the song is in the top half of hits\n","* `danceability`: how suitable a track is for dancing (0.0: least danceable, 1.0: most danceable)\n","* `energy`: perceptual measure of intensity and activity (0.0 - 1.0)\n","* `key`: the key the track is in; integers map to pitches using standard Pitch Class notation (0: C, 1: Câ™¯/Dâ™­, 2:D, ..., 11: B)\n","* `loudness`: the overall loudness of a track in decibels (dB)\n","* `mode`: the modality of a track, or the type of scale from which its melodic content is derived (0: minor, 1: major)\n","* `speechiness`: a measure of the presence of spoken words in the track (0-0.33: music and other non-speech-like tracks, 0.33-0.66: contain both music and speech, 0.66-1.0: most likely made entirely of spoken words (e.g. talk show, audio book, poetry))\n","* `acousticness`: a confidence measure of whether or not the track is acoustic (0.0: low confidence, 1.0: high confidence)\n","* `instrumentalness`: predicts whether or not a track contains vocals (0.0: vocal content, 1.0: no vocal content)\n","* `liveness`: detects the presence of an audience in the recording ( > 0.8: strong likelihood the track was performed live)\n","* `valence`: musical positiveness conveyed by the track (lower valence: more negative, higher valence: more positive)\n","* `tempo`: the overall estimated tempo in beats per minute (BPM)\n","* `genre`: the genre in which the track belongs\n","* `explicit`: whether or not the song is explicit\n","* `explicity binary`: whether or not the song is explicit (0: no, 1: yes)\n","\n","#### **Your Task**\n","Using the Spotify dataset, you will do the following:\n","* Create a SVM model that can predict whether a song will be a hit or a bust;\n","* Predict whether songs with various keys and energies will be hits or busts."],"metadata":{"id":"QgY4MTFvbXSy"}},{"cell_type":"code","source":["url = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vQJ9UIsI2j8vPnefdBj6GIrUGiDMsF5HRVAg4rsfaZqX5fAoTGLGydLvPXPQvE5ZSo9_aet1SC5UQji/pub?gid=1132556054&single=true&output=csv\"\n","df_spotify = pd.read_csv(url)\n","\n","df_spotify.head()"],"metadata":{"id":"h60VOkOybXt5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**2. Decide independent and dependent variables**"],"metadata":{"id":"ygzFapmjbXxd"}},{"cell_type":"code","source":["features = df_spotify[['key', 'energy']]\n","label = df_spotify[\"top half\"]\n","\n","plt.figure(figsize=(10,6))\n","plt.scatter(features['key'], features['energy'], c = label)\n","\n","# yellow: top hit, purple: bottom hit\n","plt.title(\"Energy vs. Key of Hit Songs Colored by Whether they were a Top or Bottom Hit\")\n","plt.xlabel(\"Key\")\n","plt.ylabel(\"Energy\")\n","\n","plt.show()"],"metadata":{"id":"MTN6-WH4dwpO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **3: Split data into training and testing data**\n"],"metadata":{"id":"W2vCtgDGdw9c"}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = model_selection.train_test_split(features, label, test_size=0.2, random_state=42)"],"metadata":{"id":"wLKGHS-NdxA_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **4. Build a SVM model and test it**"],"metadata":{"id":"bu3nLP_ybX1K"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","model = SVC()\n","model.fit(X_train, y_train)\n","pred=model.predict(X_test)"],"metadata":{"id":"ET_MaQ7wbX4a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(classification_report(y_test, pred))"],"metadata":{"id":"vOk-9vxfxC39"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cm=confusion_matrix(y_test, pred)\n","disp=ConfusionMatrixDisplay(cm, display_labels=['top half','bottom half'])\n","disp.plot()"],"metadata":{"id":"qIbs34zpMuPA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**5. Build a KNN model (with k=3) and test it**"],"metadata":{"id":"4SB0Thv7xf82"}},{"cell_type":"code","source":["knn=KNeighborsClassifier(n_neighbors=3)\n","knn.fit(X_train, y_train)\n","pred_knn=knn.predict(X_test)"],"metadata":{"id":"nBlx1AmYxhT6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(classification_report(y_test, pred_knn))"],"metadata":{"id":"JwitrpTvxhaK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **6. Use the model**\n","Use the model to predict whether the following songs are in the top hits.\n","\n","1. A song with `key = 3` and `energy = 0.8`. According to our KNN model, will this song be in the top half of hits?\n","2. A song with `key = 4.5` and `energy = 0.45`. Will this song be a bust or a hit?\n","3. A song with `key = 1` and `energy = 0.5`. Will this song be a bust or a hit?"],"metadata":{"id":"oc0mcKuNMuSs"}},{"cell_type":"code","source":["songs = pd.DataFrame([[3, 0.8], [4.5, 0.45], [1, 0.5]], columns = [\"key\", \"energy\"])\n","prediction = model.predict(songs)\n","print(prediction)"],"metadata":{"id":"OEQRxG6lMuVx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **7. Reflection Qeustion**\n","What is the best n_neighbors value? We can try a set of n_neighbors and find the best one."],"metadata":{"id":"SLsxLMOu70jZ"}},{"cell_type":"code","source":["# Hyperparameter tuning\n","\n","scores = {}\n","for n in range(1,50,2):\n","    full_model = KNeighborsClassifier(n_neighbors = n)\n","   # full_model.fit(X_train, y_train.to_numpy().reshape(-1))\n","    full_model.fit(X_train, y_train)\n","    pred = full_model.predict(X_test)\n","   # score = sum(pred == y_test.to_numpy().reshape(-1))/len(pred)* 100\n","    score = sum(pred == y_test)/len(pred)* 100\n","    scores[n] = score\n","\n","\n","plt.title(\"Accuracy on Test set across Hyperparameter values\")\n","print(scores)\n","plt.plot(list(scores.keys()), list(scores.values()), label = 'Scores for all K')\n","\n","# ADDING THE PERFORMANCE FOR K = SQRT SIZE FOR REFERENCE\n","k = int(len(X_train)**(1/2)/2)*2 - 1\n","full_model = KNeighborsClassifier(n_neighbors = k)\n","full_model.fit(X_train, y_train)\n","pred = full_model.predict(X_test)\n","score = sum(pred == y_test)/len(pred)* 100\n","plt.scatter([k], [score], color = 'r', marker = '*', s = 200, label = 'Square Root of Training Data Size')\n","\n","\n","top_score = max(scores.values())\n","best_k = list(scores.keys())[list(scores.values()).index(top_score)]\n","plt.scatter([best_k], [top_score], color = 'g', marker = '*', s = 200, label = 'Best Perfomance')\n","\n","plt.legend()\n","plt.show()\n","\n","\n","\n","# PRINTING THE RESULTS\n","print(\"Top score of optimal classifier: \" + str(top_score))\n","print(\"Best Value of K to use \" + str(best_k))"],"metadata":{"id":"0AwOnQDnMuZM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**7. Try a decision tree and see how it performs?**"],"metadata":{"id":"Rg20gHSOPR-y"}},{"cell_type":"code","source":[],"metadata":{"id":"pR6UrLWmTNGM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"e7Ec6KlHTNK0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p32\"></a>\n","## Hyperparameter Tunning for KNN\n","* Machine learning models are not intelligent enough to know what hyperparameters would lead to the highest possible accuracy on the given dataset. We can let the model try different combinations of hyperparameters during the training process and make predictions with the best combination of hyperparameter values\n","* As it's time consuming and sometimes impossible to try all possible combinations of hyperparameters, two strategies are used to quickly find the local optimal ones.\n"," * Grid Search:exhaustively searches all combinations within a predefined grid\n"," * Random Search:randomly samples a fixed number of parameter settings from specified distributions\n"],"metadata":{"id":"Ytap_mhP8iAy"}},{"cell_type":"markdown","source":["###**Grid Search**"],"metadata":{"id":"FDkvQEYfPKwe"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n","\n","kf=KFold(n_splits=10, shuffle=True, random_state=42)\n","\n","# define the range of hyper parameters\n","parameter={'n_neighbors': np.arange(2, 30, 1)}\n","\n","knn=KNeighborsClassifier()\n","\n","# define grid search strategies and scope\n","knn_cv=GridSearchCV(knn, param_grid=parameter, cv=kf, verbose=1)\n","\n","knn_cv.fit(X_train, y_train)\n","\n","# print the best hyper parameter\n","\n","print('\\nthe best hyper parameter n_neighbors: ', knn_cv.best_estimator_)\n","\n","pred=knn_cv.predict(X_test)\n","print(classification_report(y_test, pred))"],"metadata":{"id":"vOeH-3nv-tHG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Random Search**"],"metadata":{"id":"Gc7t82UnPSCN"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n","kf=KFold(n_splits=10, shuffle=True, random_state=42)\n","\n","parameter={'n_neighbors': np.arange(2, 30, 1)}\n","\n","knn=KNeighborsClassifier()\n","\n","# define random search strategies and scope\n","knn_cv=RandomizedSearchCV(knn, param_distributions=parameter, cv=kf, verbose=1)\n","\n","knn_cv.fit(X_train, y_train)\n","\n","# print the best hyper parameter\n","print('\\nthe best hyper parameter n_neighbors: ', knn_cv.best_estimator_)\n","\n","\n","pred=knn_cv.predict(X_test)\n","print(classification_report(y_test, pred))"],"metadata":{"id":"_ISLWXFx-thO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"isC1FeeX3tVa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qEB42FJy3fkJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yY4JbHQw3fpY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p33\"> </a>\n","## Hyperparameter Tunning for Decision Tree\n","A decision tree classifier has many parameters including\n","* *criterion*: functions to measure the quality of a split include *gini*, *entropy*, etc\n","* *max_depth*: the maximum depth of the tree\n","* *min_samples_split*: the minimum number of samples to split an internal node, default=2\n","* *min_samples_leaf*: the minimum number of samples at a leaf node, default=*1*"],"metadata":{"id":"7woGIUn4-tku"}},{"cell_type":"markdown","source":["###**Grid Search**"],"metadata":{"id":"1sLR7YEfUYDd"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","kf=KFold(n_splits=10, shuffle=True, random_state=42)\n","parameter={'max_depth':np.arange(3, 20, 1),'min_samples_split': np.arange(5, 50, 2)}\n","dt=DecisionTreeClassifier()\n","dt_gs=GridSearchCV(dt, param_grid=parameter, cv=kf, verbose=1)\n","dt_gs.fit(X_train, y_train)\n","\n","print(dt_gs.best_estimator_)\n","\n","pred=dt_gs.predict(X_test)\n","print(classification_report(y_test, pred))"],"metadata":{"id":"Ly9O_d3kPS4q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(dt_gs.best_estimator_)\n","\n","pred=dt_gs.predict(X_test)\n","print(classification_report(y_test, pred))"],"metadata":{"id":"CqraDl0VTUkf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Random Search**"],"metadata":{"id":"Uual_a6PPS8T"}},{"cell_type":"code","source":["kf=KFold(n_splits=10, shuffle=True, random_state=42)\n","parameter={'max_depth':np.arange(2, 20, 1),'min_samples_split': np.arange(4, 50, 2)}\n","dt=DecisionTreeClassifier()\n","dt_rs=RandomizedSearchCV(dt, param_distributions=parameter, cv=kf, verbose=1)\n","dt_rs.fit(X_train, y_train)\n","\n","print(dt_rs.best_estimator_)\n","\n","pred=dt_rs.predict(X_test)\n","print(classification_report(y_test, pred))"],"metadata":{"id":"qg1T5NT3PTAd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kdo_gimL-toO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p34\"> </a>\n","## Hyperparameter Tunning for Naive Bayes Classifier\n","[Naive Bayes](https://www.datacamp.com/tutorial/naive-bayes-scikit-learn)"],"metadata":{"id":"wnp5SBan-trn"}},{"cell_type":"markdown","source":["### **Grid Search**"],"metadata":{"id":"5CXQopi39yTH"}},{"cell_type":"code","source":["from sklearn.naive_bayes import GaussianNB\n","nb=GaussianNB()\n","params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n","NB_gs = GridSearchCV(estimator=nb,\n","                 param_grid=params_NB,\n","                 cv=kf,   # use any cross validation technique\n","                 verbose=1)\n","NB_gs.fit(X_train, y_train)\n","\n","print(NB_gs.best_estimator_)\n","pred=NB_gs.predict(X_test)\n","print(classification_report(y_test,pred ))"],"metadata":{"id":"jMP4KGmD8iD_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(NB_gs.best_estimator_)"],"metadata":{"id":"OzShvPzw76bV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred=NB_gs.predict(X_test)\n","print(classification_report(y_test,pred ))"],"metadata":{"id":"Fb3IwAb8-AjA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Random Search**"],"metadata":{"id":"QL2pqpIr97bi"}},{"cell_type":"code","source":["from scipy.stats import loguniform\n","from sklearn.naive_bayes import GaussianNB\n","nb=GaussianNB()\n","params_NB = {'var_smoothing': loguniform(1e0,1e-9)}\n","NB_rs = RandomizedSearchCV(estimator=nb,\n","                 param_distributions=params_NB,\n","                 cv=kf,   # use any cross validation technique\n","                 verbose=1,\n","                 scoring='accuracy')\n","NB_rs.fit(X_train, y_train)\n","\n","pred=NB_rs.predict(X_test)\n","\n","print(NB_gs.best_estimator_)\n","print(classification_report(y_test, pred))"],"metadata":{"id":"cvpYfA2T97Xr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sKv1rVAa97Tg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uu7MBZVp97NM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p35\"> </a>\n","## Hyperparameter Tunning for Support Vector Machine (SVM)"],"metadata":{"id":"fZuoSFDv8iHh"}},{"cell_type":"markdown","source":["### **Grid Search**"],"metadata":{"id":"h0ZgQpCd8iN_"}},{"cell_type":"code","source":["from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n","from scipy.stats import uniform\n","from sklearn.svm import SVC\n","svm=SVC()\n","param_dist = {\n","    'C': [0.1, 1, 10],\n","    'kernel': ['linear', 'rbf', 'poly'],\n","    'gamma': ['scale', 'auto']\n","} # total 3*3*2 combinations\n","SVM_rs = RandomizedSearchCV(estimator=svm, param_distributions=param_dist, n_iter=20, cv=5)\n","\n","SVM_rs.fit(X_train, y_train)\n","\n","\n","pred=SVM_rs.predict(X_test)\n","\n","print(SVM_rs.best_estimator_)\n","print(classification_report(y_test, pred))"],"metadata":{"id":"56M6Tcc-8iRM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Random Search**"],"metadata":{"id":"mYyYu2nl8iUn"}},{"cell_type":"code","source":["from scipy.stats import uniform\n","from sklearn.svm import SVC\n","svm=SVC()\n","param_dist = {\n","    'C': uniform(0.1, 10),  # Uniform distribution between 0.1 and 10\n","    'kernel': ['linear', 'rbf', 'poly'],\n","    'gamma': ['scale', 'auto'] + list(np.logspace(-3, 3, 50))\n","}\n","SVM_rs = RandomizedSearchCV(estimator=svm, param_distributions=param_dist, n_iter=20, cv=5)\n","\n","SVM_rs.fit(X_train, y_train)\n","\n","\n","pred=SVM_rs.predict(X_test)\n","\n","print(SVM_rs.best_estimator_)\n","print(classification_report(y_test, pred))"],"metadata":{"id":"S8xystyh8iKs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Follow up\n","Only two columns ('key' and 'energy') were used in predicting whether a song will be a hit or not. This might be a reason why that classifiers didn't perform well.\n","\n","Try different strategies and see if you can build a classifier with a better accuracy."],"metadata":{"id":"g3PPFi-0Muc2"}},{"cell_type":"code","source":[],"metadata":{"id":"xVlhy83UWXCF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PFUO2ts5WXFw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_y2jKQIbWXJs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p5\"> </a>\n","# **Part 5-- Exercise**\n","Use diabetes dataset (df_diabetes) in this exercise. Build the following classifiers with hyper parameter tuning:\n","*   Decision Tree\n","*   SVM\n","*   KNN\n"],"metadata":{"id":"4NIXKMfRENSW"}},{"cell_type":"code","source":[],"metadata":{"id":"dRGY1fk2EUQJ"},"execution_count":null,"outputs":[]}]}